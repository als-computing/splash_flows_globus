# Beamline 7.3.3 Flows

This page documents the workflows supported by Splash Flows Globus at [ALS Beamline 7.3.3 (SAXS/WAXS/GISAXS)](https://saxswaxs.lbl.gov/user-information).

## Diagrams

### Sequence Diagram
```mermaid
sequenceDiagram
    %% Participants
    participant Detector as Detector
    participant FileWatcher as File Watcher (data733)
    participant Dispatcher as Dispatcher [Prefect Worker]
    
    %% Flow 1: new_file_733 Prefect Flow
    participant F1_Data as Flow1: data733
    participant F1_CFS as Flow1: NERSC CFS
    participant F1_SciCat as Flow1: SciCat (Metadata DB)
    
    %% Flow 2: Scheduled HPSS Transfer Prefect Flow
    participant F2_CFS as Flow2: NERSC CFS
    participant F2_HPSS as Flow2: HPSS Tape Archive
    participant F2_SciCat as Flow2: SciCat (Metadata DB)
    
    %% Flow 3: HPC Downstream Analysis Prefect Flow
    participant F3_Data as Flow3: data733
    participant HPC_FS as HPC Filesystem
    participant HPC_Compute as HPC Compute
    
    %% Scheduled Pruning (triggered by all flows)
    participant SPruning as Scheduled Pruning (Prefect Workers)
    participant P_CFS as Prune Target: NERSC CFS
    participant P_Data as Prune Target: data733

    %% Initial Trigger Sequence
    Detector->>FileWatcher: Send Raw Data
    FileWatcher->>Dispatcher: File Watcher Trigger
    Dispatcher->>F1_Data: Start new_file_733 Flow
    Dispatcher->>F2_CFS: Start Scheduled HPSS Transfer Flow
    Dispatcher->>F3_Data: Start HPC Downstream Analysis Flow

    %% Flow 1 interactions
    F1_Data->>F1_CFS: Globus Transfer: Raw Data
    F1_CFS->>F1_SciCat: SciCat Ingestion: Metadata

    %% Flow 2 interactions
    F2_CFS->>F2_HPSS: SFAPI Slurm htar Transfer: Raw Data
    F2_HPSS->>F2_SciCat: SciCat Ingestion: Metadata

    %% Flow 3 interactions
    F3_Data->>HPC_FS: Globus Transfer: Raw Data
    HPC_FS->>HPC_Compute: Transfer Raw Data
    HPC_Compute->>HPC_FS: Return Scratch Data
    HPC_FS->>F3_Data: Globus Transfer: Scratch Data

    %% Scheduled Pruning triggered by flows
    F1_SciCat-->>SPruning: Trigger Pruning
    F2_SciCat-->>SPruning: Trigger Pruning
    F3_Data-->>SPruning: Trigger Pruning

    SPruning->>P_CFS: Prune NERSC CFS
    SPruning->>P_Data: Prune data733
```

### Data Infrastructure Workflows
```mermaid
---
config:
  theme: neo
  layout: elk
  look: neo
---
flowchart LR
 subgraph s1["new_file_733<br>[Prefect Flow]"]
        n20["data733"]
        n21["NERSC CFS"]
        n22@{ label: "SciCat<br style=\"--tw-scale-x:\">[Metadata Database]" }
  end
 subgraph s2["Scheduled HPSS Transfer<br>[Prefect Flow]"]
        n38["NERSC CFS"]
        n39["HPSS Tape Archive"]
        n40["SciCat <br>[Metadata Database]"]
  end
 subgraph s3["HPC Downstream Analysis<br>[Prefect Flow]"]
        n41["data733"]
        n42["HPC<br>Filesystem"]
        n43["HPC<br>Compute"]
  end
    n23["data733"] -- File Watcher --> n24["Dispatcher<br>[Prefect Worker]"]
    n25["Detector"] -- Raw Data --> n23
    n24 --> s1 & s2 & s3
    n20 -- Raw Data [Globus Transfer] --> n21
    n21 -- "<span style=color:>Metadata [SciCat Ingestion]</span>" --> n22
    n32["Scheduled Pruning <br>[Prefect Workers]"] --> n35["NERSC CFS"] & n34["data733"]
    n38 -- Raw Data [SFAPI Slurm htar Transfer] --> n39
    n39 -- "<span style=color:>Metadata [SciCat Ingestion]</span>" --> n40
    s2 --> n32
    s3 --> n32
    s1 --> n32
    n41 -- Raw Data [Globus Transfer] --> n42
    n42 -- Raw Data --> n43
    n43 -- Scratch Data --> n42
    n42 -- Scratch Data [Globus Transfer] --> n41
    n20@{ shape: internal-storage}
    n21@{ shape: disk}
    n22@{ shape: db}
    n38@{ shape: disk}
    n39@{ shape: paper-tape}
    n40@{ shape: db}
    n41@{ shape: internal-storage}
    n42@{ shape: disk}
    n23@{ shape: internal-storage}
    n24@{ shape: rect}
    n25@{ shape: rounded}
    n35@{ shape: disk}
    n34@{ shape: internal-storage}
     n20:::storage
     n20:::Peach
     n21:::Sky
     n22:::Sky
     n38:::Sky
     n39:::storage
     n40:::Sky
     n41:::Peach
     n42:::Sky
     n43:::compute
     n23:::collection
     n23:::storage
     n23:::Peach
     n24:::collection
     n24:::Rose
     n25:::Ash
     n32:::Rose
     n35:::Sky
     n34:::Peach
    classDef collection fill:#D3A6A1, stroke:#D3A6A1, stroke-width:2px, color:#000000
    classDef Rose stroke-width:1px, stroke-dasharray:none, stroke:#FF5978, fill:#FFDFE5, color:#8E2236
    classDef storage fill:#A3C1DA, stroke:#A3C1DA, stroke-width:2px, color:#000000
    classDef Ash stroke-width:1px, stroke-dasharray:none, stroke:#999999, fill:#EEEEEE, color:#000000
    classDef visualization fill:#E8D5A6, stroke:#E8D5A6, stroke-width:2px, color:#000000
    classDef Peach stroke-width:1px, stroke-dasharray:none, stroke:#FBB35A, fill:#FFEFDB, color:#8F632D
    classDef Sky stroke-width:1px, stroke-dasharray:none, stroke:#374D7C, fill:#E2EBFF, color:#374D7C
    classDef compute fill:#A9C0C9, stroke:#A9C0C9, stroke-width:2px, color:#000000
    style s1 stroke:#757575
    style s2 stroke:#757575
    style s3 stroke:#757575

```

## File Watcher

There is a file watcher on the system `data733` that listens for new scans that have finished writing to disk. From there, a Prefect Flow we call `dispatcher` kicks off the downstream steps:
- Copy scans in real time to `NERSC CFS` using Globus Transfer.
- Copy project data to `NERSC HPSS` for long-term storage.
- Analysis on HPC systems (TBD).
- Schedule data pruning from `data733` and `NERSC CFS`.

## Prefect Configuration

### Registered Flows

#### `dispatcher.py`

#### `move.py`

## VM Details

The computing backend runs on a VM in the B15 server room that is managed by ALS IT staff.

**Name**: `flow-saxs-waxs`
**OS**: `Ubuntu 24.04 LTS`