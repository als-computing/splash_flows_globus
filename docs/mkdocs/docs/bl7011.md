# Beamline 7.0.1.1 Flows

This page documents the workflows supported by Splash Flows Globus at [ALS Beamline 7.0.1.1 (COSMIC Scattering)](https://als.lbl.gov/beamlines/7-0-1-1/).

## Data at 7.0.1.1

At Beamline 7.0.1.1, users generate data in an HDF5 format containing a background subtracted stack of 2D images with associated Labview metadata. Depending on the experiment, the file sizes can be greater than 100GB. A ROI is exported for each dataset.

## File Watcher

There is a file watcher on the system `QNAP` that listens for new scans that have finished writing to disk. From there, a Prefect Flow we call `dispatcher` kicks off the downstream steps:
- Copy scans in real time to `NERSC CFS` using Globus Transfer.
- Copy project data to `NERSC HPSS` for long-term storage.
- Analysis on HPC systems (TBD).
- Schedule data pruning from `QNAP` and `NERSC CFS`.


## Prefect Configuration

### Registered Flows

#### `dispatcher.py`

The Dispatcher Prefect Flow manages the logic for handling the order and execution of data tasks. As as soon as the File Watcher detects that a new file is written, it calls the `dispatcher()` Flow. In this case, the dispatcher handles the synchronous call to `move.py`, with a potential to add additional steps (e.g. scheduling remote HPC analysis code).

#### `move.py`

Flow to process a new file at BL 7.0.1.1
1. Copy the file from the QNAP to NERSC CFS. Ingest file path in SciCat.
2. Schedule pruning from QNAP.
3. Copy the file from NERSC CFS to NERSC HPSS. Ingest file path in SciCat.
4. Schedule pruning from NERSC CFS.


## VM Details

The computing backend runs on a VM in the B15 server room that is managed by ALS IT staff.

**Name**: `flow-xpcs`
**OS**: `Ubuntu 20.02 LTS` ... **must be updated to `Ubuntu 24.04 LTS`**