{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Splash Flows Globus","text":"<p>This repository contains code for automating workflows at several Advanced Light Source beamlines.</p>"},{"location":"#project-layout","title":"Project layout","text":"<p>Here is the current structure of this repository:</p> <pre><code>.\n\u2514\u2500\u2500 splash_flows_globus\n    \u251c\u2500\u2500 COPYWRITE\n    \u251c\u2500\u2500 Dockerfile\n    \u251c\u2500\u2500 LICENSE\n    \u251c\u2500\u2500 Makefile\n    \u251c\u2500\u2500 README.md\n    \u251c\u2500\u2500 config.yml\n    \u251c\u2500\u2500 create_deployment_832_dispatcher.sh\n    \u251c\u2500\u2500 create_deployments_7012.sh\n    \u251c\u2500\u2500 create_deployments_832.sh\n    \u251c\u2500\u2500 create_deployments_832_alcf.sh\n    \u251c\u2500\u2500 create_deployments_832_nersc.sh\n    \u251c\u2500\u2500 docs\n    \u2502   \u251c\u2500\u2500 bl7012.md\n    \u2502   \u251c\u2500\u2500 bl832_ALCF.md\n    \u2502   \u251c\u2500\u2500 globus.md\n    \u2502   \u2514\u2500\u2500 mkdocs\n    \u2502       \u251c\u2500\u2500 docs\n    \u2502       \u2502   \u251c\u2500\u2500 configuration.md\n    \u2502       \u2502   \u251c\u2500\u2500 getting_started.md\n    \u2502       \u2502   \u251c\u2500\u2500 glossary.md\n    \u2502       \u2502   \u251c\u2500\u2500 index.md\n    \u2502       \u2502   \u251c\u2500\u2500 install.md\n    \u2502       \u2502   \u251c\u2500\u2500 orchestration.md\n    \u2502       \u2502   \u2514\u2500\u2500 troubleshooting.md\n    \u2502       \u2514\u2500\u2500 mkdocs.yml\n    \u251c\u2500\u2500 examples\n    \u2502   \u251c\u2500\u2500 launch_ptycho.py\n    \u2502   \u251c\u2500\u2500 sfapi_NerscClient_example.ipynb\n    \u2502   \u2514\u2500\u2500 start_job.ipynb\n    \u251c\u2500\u2500 orchestration\n    \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 _tests\n    \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u2502   \u251c\u2500\u2500 conftest.py\n    \u2502   \u2502   \u251c\u2500\u2500 test_config.py\n    \u2502   \u2502   \u251c\u2500\u2500 test_config.yml\n    \u2502   \u2502   \u251c\u2500\u2500 test_globus.py\n    \u2502   \u2502   \u251c\u2500\u2500 test_globus_flow.py\n    \u2502   \u2502   \u251c\u2500\u2500 test_scicat.py\n    \u2502   \u2502   \u251c\u2500\u2500 test_sfapi_flow.py\n    \u2502   \u2502   \u2514\u2500\u2500 test_transfer_controller.py\n    \u2502   \u251c\u2500\u2500 config.py\n    \u2502   \u251c\u2500\u2500 flows\n    \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u2502   \u251c\u2500\u2500 bl7012\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 config.py\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 example.ipynb\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 move.py\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 move_recon.py\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 ptycho_jobscript.py\n    \u2502   \u2502   \u2502   \u2514\u2500\u2500 ptycho_nersc.py\n    \u2502   \u2502   \u251c\u2500\u2500 bl733\n    \u2502   \u2502   \u2502   \u2514\u2500\u2500 move_733.py\n    \u2502   \u2502   \u251c\u2500\u2500 bl832\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 alcf.py\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 config.py\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 dispatcher.py\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 ingest_tomo832.py\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 job_controller.py\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 move.py\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 nersc.py\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 olcf.py\n    \u2502   \u2502   \u2502   \u2514\u2500\u2500 prune.py\n    \u2502   \u2502   \u2514\u2500\u2500 scicat\n    \u2502   \u2502       \u251c\u2500\u2500 __init__.py\n    \u2502   \u2502       \u251c\u2500\u2500 ingest.py\n    \u2502   \u2502       \u2514\u2500\u2500 utils.py\n    \u2502   \u251c\u2500\u2500 globus\n    \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u2502   \u251c\u2500\u2500 flows.py\n    \u2502   \u2502   \u2514\u2500\u2500 transfer.py\n    \u2502   \u251c\u2500\u2500 nersc.py\n    \u2502   \u251c\u2500\u2500 prefect.py\n    \u2502   \u2514\u2500\u2500 transfer_controller.py\n    \u251c\u2500\u2500 pytest.ini\n    \u251c\u2500\u2500 requirements-dev.txt\n    \u251c\u2500\u2500 requirements.txt\n    \u251c\u2500\u2500 scripts\n    \u2502   \u251c\u2500\u2500 Tomopy_for_ALS.ipynb\n    \u2502   \u251c\u2500\u2500 cancel_sfapi_job.py\n    \u2502   \u251c\u2500\u2500 check_globus_compute.py\n    \u2502   \u251c\u2500\u2500 check_globus_transfer.py\n    \u2502   \u251c\u2500\u2500 init_tiff_to_zarr_globus_flow.py\n    \u2502   \u251c\u2500\u2500 init_tomopy_globus_flow.py\n    \u2502   \u251c\u2500\u2500 login_to_globus_and_prefect.sh\n    \u2502   \u2514\u2500\u2500 polaris\n    \u2502       \u251c\u2500\u2500 globus_reconstruction.py\n    \u2502       \u2514\u2500\u2500 tiff_to_zarr.py\n    \u251c\u2500\u2500 setup.cfg\n    \u2514\u2500\u2500 setup.py\n</code></pre>"},{"location":"about/","title":"About","text":"<p>This software is created and maintained by the ALS Computing Group and subject to change.</p>"},{"location":"alcf832/","title":"Run Tomography Reconstruction Remotely at ALCF","text":"<p>This document outlines the steps required to set up and run Globus Transfer and Compute Flows for transferring and processing raw tomography data from Beamline 8.3.2 at Berkeley Lab's Advanced Light Source (ALS) to the Argonne Leadership Computing Facility (ALCF). This script relies on Prefect for workflow orchestration, Globus Transfer for data movement between facilities, and Globus Compute at ALCF for tomographic reconstruction using Tomopy.</p> <p>Follow these steps to log in to ALCF Polaris, start a Globus compute endpoint signed in using a Globus confidential client, register a reconstruction function and flow, and run the flow using <code>orchestration/flows/bl832/alcf.py</code>.</p>"},{"location":"alcf832/#index","title":"Index","text":"<ul> <li>Details<ul> <li>Data flow diagram</li> <li>File flow details </li> <li>Globus Compute Flow</li> <li>Reconstruction</li> <li>Zarr Generation</li> <li>Pruning </li> </ul> </li> <li>Set up <ul> <li>Requirements </li> <li>On Polaris </li> <li>In your local environment </li> <li>Register Globus Compute Functions and Flows</li> <li>Schedule Pruning with Prefect Workers</li> </ul> </li> <li>Helper Scripts</li> <li>Performance</li> </ul>"},{"location":"alcf832/#details","title":"Details","text":""},{"location":"alcf832/#data-flow-diagram","title":"Data flow diagram","text":"<pre><code>%%{\ninit: {\n  \"theme\": \"base\",\n  \"themeVariables\": {\n    \"primaryColor\": \"#000000\",\n    \"primaryTextColor\": \"#000000\",\n    \"primaryBorderColor\": \"#000000\",\n    \"lineColor\": \"#808080\", // Gray arrows\n    \"arrowheadColor\": \"#ffffff\",\n    \"secondaryColor\": \"#00000\",\n    \"tertiaryColor\": \"#ffffff\",\n    \"fontFamily\": \"Arial, Helvetica, sans-serif\",\n    \"background\": \"#ffffff\",\n  }\n}\n}%%\ngraph TD\n    A[Beamline 8.3.2] --&gt;|Collect Data| B[spot832]\n    B --&gt;|Globus Transfer to data832| C[ /data/raw/&lt; experiment folder name convention&gt;/&lt; h5 files&gt;]\n    C --&gt;|Start This Prefect Flow| D[ python orchestration/flows/bl832/alcf.py ]\n    D --&gt;|Step 1: Globus Transfer from data832 to ALCF| E[ /eagle/IRIBeta/als/bl832/raw/&lt; experiment folder name convention&gt;/&lt; h5 files&gt;]\n    E --&gt;|Step 2: Run Globus Flows on ALCF Globus Compute Endpoint| F[ A: Run globus_reconstruction.py &lt;br&gt; B: Run tiff_to_zarr.py]\n    F --&gt;|Save Reconstruction on ALCF| G[ /eagle/IRIBeta/als/bl832/scratch/&lt; experiment folder name convention&gt;/rec&lt; dataset&gt;/&lt; tiffs&gt; &lt;br&gt; /eagle/IRIBeta/als/bl832/scratch/&lt; experiment folder name convention&gt;/rec&lt; dataset&gt;.zarr/ ]\n    G --&gt;|Step 3: Globus Transfer back to data832| J[ /data/scratch/globus_share/&lt; experiment folder name convention&gt;/rec&lt; dataset&gt;/&lt; tiffs&gt; &lt;br&gt; /data/scratch/globus_share/&lt; experiment folder name convention&gt;/rec&lt; dataset&gt;.zarr/ ]\n    J --&gt;|Schedule Pruning| L[Prune raw and scratch data from ALCF and data832]\n    J --&gt; M[Visualize Results] \n    J --&gt;|SciCat| N[TODO: ingest metadata into SciCat]\n    M --&gt;|2D Image Slices| O[view .tiff images ]\n    M --&gt;|3D Visualization| P[load .zarr directory in view_tiff.ipynb]\n\n    classDef storage fill:#A3C1DA,stroke:#A3C1DA,stroke-width:2px,color:#000000;\n    class C,E,G,J storage;\n\n    classDef collection fill:#D3A6A1,stroke:#D3A6A1,stroke-width:2px,color:#000000;\n    class A,B collection;\n\n    classDef compute fill:#A9C0C9,stroke:#A9C0C9,stroke-width:2px,color:#000000;\n    class D,F,K,L,N compute;\n\n    classDef visualization fill:#E8D5A6,stroke:#E8D5A6,stroke-width:2px,color:#000000;\n    class M,O,P visualization;\n</code></pre>"},{"location":"alcf832/#file-flow-details","title":"File flow details","text":"<p>Naming conventions -   Experiment folder name:     - <code>&lt; Proposal Prefix&gt;-&lt; 5 digit proposal number&gt;_&lt; first part of email address&gt;/</code> -   h5 files in experiment folder:     - <code>&lt; YYYYMMDD&gt;_&lt; HHMMSS&gt;_&lt; user defined string&gt;.h5</code> -   h5 files for tiled scans:     - <code>&lt; YYYYMMDD&gt;_&lt; HHMMSS&gt;_&lt; user defined string&gt;_x&lt;##&gt;y&lt;##&gt;.h5</code></p> <p>data832 Raw Location - <code>data/raw/&lt; Experiment folder name convention&gt;/&lt; h5 files&gt;</code></p> <p>data832 Scratch Location - <code>data/scratch/globus_share/&lt; Experiment folder name convention&gt;/rec&lt; dataset&gt;/&lt; tiffs&gt;</code> - <code>data/scratch/globus_share/&lt; Experiment folder name convention&gt;/rec&lt; dataset&gt;.zarr/</code></p> <p>ALCF Raw Location - <code>/eagle/IRIBeta/als/bl832/raw/&lt; Experiment folder name convention&gt;/&lt; h5 files&gt;</code></p> <p>ALCF Scratch Location - <code>/eagle/IRIBeta/als/bl832/scratch/&lt; Experiment folder name convention&gt;/rec&lt; dataset&gt;/&lt; tiffs&gt;</code> - <code>/eagle/IRIBeta/als/bl832/scratch/&lt; Experiment folder name convention&gt;/rec&lt; dataset&gt;.zarr/</code></p>"},{"location":"alcf832/#globus-compute-flows","title":"Globus Compute Flows","text":"<p>The preferred method for scheduling jobs on ALCF is through Globus Compute. This is done by registering a function containing the code to run with Globus Compute Flows, which will then be called when running the reconstruction on ALCF. Two scripts are included in this repository that set up reconstruction and Zarr conversion Globus Compute Flows:</p> <ul> <li>Reconstruction<ul> <li><code>/scripts/init_reconstruction_globus_flow.py</code></li> <li><code>reconstruction_wrapper()</code> defines the function registered with Globus Compute.</li> </ul> </li> <li>Tiff to Zarr<ul> <li><code>/scripts/init_tiff_to_zarr_globus_flow.py</code></li> <li><code>conversion_wrapper()</code> defines the function registered with Globus Compute.</li> </ul> </li> </ul>"},{"location":"alcf832/#reconstruction","title":"Reconstruction","text":"<p>The <code>globus_reconstruction.py</code> script in this flow is a slightly modified version of Dula Parkinson's tomography reconstruction code.  The modifications update the function's input parameters and set proper file/folder permissions for the reconstructed data.</p> <ul> <li>Location in this repo: <code>/scripts/polaris/globus_reconstruction.py</code></li> <li>Location on Polaris: <code>/eagle/IRIBeta/als/example/globus_reconstruction.py</code></li> </ul>"},{"location":"alcf832/#zarr-conversion","title":"Zarr Conversion","text":"<p>Once the raw data has been reconstructed into a series of .tiff images, the script <code>tiff_to_zarr.py</code> transforms the images into a multiresolution .zarr directory, enabling 3D visualization. This code was adapted from the <code>view_tiff.ipynb</code> notebook and converted into a Python module. </p> <ul> <li>Location in this repo: <code>/scripts/polaris/tiff_to_zarr.py</code></li> <li>Location on Polaris: <code>/eagle/IRIBeta/als/example/tiff_to_zarr.py</code></li> </ul>"},{"location":"alcf832/#pruning","title":"Pruning","text":"<p>After reconstruction is complete and data has moved back to NERSC/ALS, Prefect flows are scheduled to delete raw scratch paths at each location after a few days. Make sure to move the reconstructed data elsewhere before pruning occurs.</p> <ul> <li>ALCF <ul> <li>Purge <code>raw</code> and <code>scratch</code> after 2 days</li> </ul> </li> <li>data832<ul> <li><code>/data/scratch/globus_share/&lt; folder name convention&gt;/rec&lt; dataset&gt;/&lt; tiffs&gt;</code></li> <li><code>/data/scratch/globus_share/&lt; folder name convention&gt;/rec&lt; dataset&gt;.zarr/</code></li> <li>Purge <code>scratch</code> after 3 days</li> </ul> </li> </ul>"},{"location":"alcf832/#set-up","title":"Set up","text":"<p>A few steps are required to configure the environments locally and on ALCF Polaris.</p>"},{"location":"alcf832/#requirements","title":"Requirements","text":"<p>Some of the steps refer to code and notebooks provided in the git repository <code>als-computing/als_at_alcf_tomopy_compute</code>. Download/clone this repository as it will come in handy</p> <p>Specifically, from here you will need: <code>Tomopy_for_ALS.ipynb</code>, and <code>template_config.yaml</code>.</p> <p>Additionally, if you do not already have an ALCF account, follow the steps here to request access.</p> <p>You will also need to update the included <code>.env.example</code> file (rename it <code>.env</code>) with particular endpoints, IDs, and secrets, which are annotated below along with the steps they correspond to:</p>"},{"location":"alcf832/#envexample","title":".env.example","text":"<pre><code># Used by Python script to sign into Globus using confidential client\nGLOBUS_CLIENT_ID=\"&lt; Client UUID from step 3 &gt;\"\nGLOBUS_CLIENT_SECRET=\"&lt; Client Secret from step 3\"\n# These set environment variables to sign into globus-compute-endpoint environment using confidential client\nGLOBUS_COMPUTE_CLIENT_ID=\"&lt; Client UUID from step 3 &gt;\"\nGLOBUS_COMPUTE_CLIENT_SECRET=\"&lt; Client Secret from step 3\"\n# globus-compute-endpoint UUID\nGLOBUS_COMPUTE_ENDPOINT=\"&lt; globus-compute-endpoint ID from step 6 &gt;\"\nGLOBUS_CGS_ENDPOINT=\"&lt; IRIBeta_als guest collection UUID on Globus &gt;\"\nPREFECT_API_KEY=\"&lt; your key for your Prefect server's API &gt;\"\nPREFECT_API_URL=\"&lt; url to your Prefect server's API &gt;\"\n</code></pre>"},{"location":"alcf832/#prefect-secret-blocks","title":"Prefect Secret Blocks","text":"<p>In addition to storing UUIDs in a <code>.env</code> file, we utilize Prefect's Secret Blocks. These can be configured in Prefect's user interface. The <code>.env</code> is great for managing secrets locally, but the Secret Blocks are handy for deployment and maintainability.</p> <ol> <li>Navigate to your Prefect server instance in production at 8.3.2, Prefect cloud, or local server.</li> <li>On the left column, navigate to \"Configuration --&gt; Blocks\"</li> <li>In the \"Blocks\" Window, hit the \"+\" plus sign next to the word \"Blocks\"</li> <li>Search for \"Secret\" and create a \"Secret Block\"</li> <li>Create the following secrets, and set their values to the corresponding UUID string:<pre><code>globus-client-id\nglobus-client-secret\nglobus-compute-endpoint\n</code></pre> </li> </ol>"},{"location":"alcf832/#on-polaris","title":"On Polaris","text":""},{"location":"alcf832/#1-ssh-into-polaris","title":"1. SSH into Polaris","text":"<p>Login to Polaris using the following terminal command:</p> <pre><code>ssh &lt;your ALCF username&gt;@polaris.alcf.anl.gov\n</code></pre> <p>Password: copy passcode from MobilePASS+</p>"},{"location":"alcf832/#2-copy-and-update-endpoint-template_configyaml-file","title":"2. Copy and Update endpoint template_config.yaml file","text":"<p>From the <code>als-computing/als_at_alcf_tomopy_compute</code> repository, copy <code>template_config.yaml</code> into your home directory on Polaris (<code>eagle/home/&lt;your ALCF username&gt;</code>).</p> <p>Ex: using vim on Polaris</p> <ul> <li> <p><code>vim template_config.yaml</code></p> </li> <li> <p>Paste file contents once the editor opens up</p> </li> <li> <p>To save, type:</p> <ul> <li><code>:wq!</code> and press enter.</li> </ul> </li> <li> <p>The view should switch back to the command line.</p> </li> </ul> <p>Note: template_config.yaml needs to be updated to account for the new version of globus-compute-endpoint:</p> <p>What you need to change is that you should replace this:  </p> <pre><code>strategy:\n    max_idletime: 300\n    type: SimpleStrategy\ntype: GlobusComputeEngine\n</code></pre> <p>with this:  </p> <pre><code>strategy: simple\njob_status_kwargs:\n    max_idletime: 300\n    strategy_period: 60\ntype: GlobusComputeEngine\n</code></pre> <p>These new settings also introduce a parameter \"strategy_period\" that controls the frequency with which the endpoint checks pbs for completed jobs. The default was 5 seconds, and this change makes it 60 seconds.</p>"},{"location":"alcf832/#3-use-an-existing-globus-confidential-client-or-create-a-new-one","title":"3. Use an existing Globus Confidential Client, or create a new one","text":"<ul> <li>In your browser, navigate to globus.org</li> <li>Login</li> <li>On the left, navigate to \"Settings\"</li> <li>On the top navigation bar, select \"Developers\" </li> <li>On the right under Projects, create a new project called Splash Flows or select an existing one</li> <li>Create a new registered app called Splash Flows App, or select an existing one</li> <li>Generate a secret</li> <li>Store the confidential client UUID and Secret (note: make sure you copy the Client UUID and not the Secret UUID)</li> </ul> <p>Note: If you create a new service client, you will need to get permissions set by the correct person at ALCF, NERSC or other facility to be able to use it for transfer endpoints.</p>"},{"location":"alcf832/#4-log-into-globus-compute-endpoint-on-polaris-with-the-service-confidential-client","title":"4. Log into <code>globus-compute-endpoint</code> on Polaris with the service confidential client","text":"<p>In your terminal on Polaris, set the following global variables with the Globus Confidential Client UUID and Secret respectively. Check the documentation here for more information (https://globus-compute.readthedocs.io/en/stable/sdk.html#client-credentials-with-clients). Globus-compute-endpoint will then log in using these credentials automatically:</p> <pre><code>export GLOBUS_COMPUTE_CLIENT_ID=\"&lt;UUID&gt;\"\nexport GLOBUS_COMPUTE_CLIENT_SECRET=\"&lt;SECRET&gt;\"\n</code></pre> <p>Note: you can make sure you are signed into the correct account by entering:</p> <pre><code>globus-compute-endpoint whoami\n</code></pre> <p>If you are signed into your personal Globus account, make sure to sign out completely using:</p> <pre><code>globus-compute-endpoint logout\n</code></pre>"},{"location":"alcf832/#5-start-globus-compute-tomopy-environment-and-activate-the-endpoint","title":"5. Start globus-compute <code>tomopy</code> environment and activate the endpoint","text":"<p>On Polaris, enter the following commands to activate a Conda environment configured to run reconstruction using <code>tomopy</code> and <code>globus-compute-endpoint</code>.</p> <pre><code>module use /soft/modulefiles\nmodule  load  conda\nconda  activate  /eagle/IRIBeta/als/env/tomopy\nglobus-compute-endpoint  configure  --endpoint-config  template_config.yaml  als_endpoint\nglobus-compute-endpoint  start  als_endpoint\nglobus-compute-endpoint  list\n</code></pre> <p>This will create an endpoint and display its status. Its status should be listed as <code>running</code>. There will also be displayed a unique Endpoint ID in the form of a UUID.</p> <p>Optional: Create a new file called <code>activate_tomopy.sh</code> in your home directory on Polaris and copy the following code:</p> <pre><code>#!/bin/bash\n#Load module files\nmodule use /soft/modulefiles\n#Load conda\nmodule load conda\n#initialize conda for the active session\nsource $(conda info --base)/etc/profile.d/conda.sh\n#Activate conda environment\nconda activate /eagle/IRI-ALS-832/env/tomopy\n#Print message\necho \"Conda environment '/eagle/IRI-ALS-832/env/tomopy' activated successfully.\"\n</code></pre> <p>Now you can call this script using <code>source activate_tomopy.sh</code> in your home directory on Polaris to activate the environment in a single line of code. Be aware that this will not configure or start any globus-compute-endpoints.</p>"},{"location":"alcf832/#6-store-endpoint-uuid-in-env-file","title":"6. Store endpoint uuid in .env file","text":"<p>Store the Endpoint UUID in your <code>.env</code> file, which is given by running: </p> <pre><code>globus-compute-endpoint list\n</code></pre> <p>This will be used for running the Flow.</p>"},{"location":"alcf832/#in-your-local-environment","title":"In your local environment","text":""},{"location":"alcf832/#7-create-a-new-conda-environment-called-globus_env-or-install-python-dependencies-directly-installtxt","title":"7. Create a new conda environment called <code>globus_env</code>, or install Python dependencies directly (install.txt)","text":"<p>The only requirement is a local environment, such as a Conda environment, that has Python 3.11 installed along with the Globus packages <code>globus_compute_sdk</code> and <code>globus_cli</code>. If you have a local installation of Conda you can set up an environment that can run the <code>scripts/Tomopy_for_ALS.ipynb</code> notebook and <code>orchestration/flows/bl832/alcf.py</code> with these steps in your terminal:</p> <p>If you haven't made a new Conda environment for this project, go ahead and make one.     conda  create  -n  globus_env  python==3.11</p> <p>Either way, make sure your environment is activated:</p> <pre><code>conda  activate  globus_env\n</code></pre> <p>And then install these required packages if you haven't already:</p> <pre><code>pip  install  globus_compute_sdk  globus_cli  python-dotenv\n</code></pre> <p>Note that the tomopy environment on Polaris contains Python 3.11. Therefore your local environment must have a Python version close to this version.</p>"},{"location":"alcf832/#8-prefect-server","title":"8. Prefect server","text":"<p>Make sure you have Prefect setup. If you want to connect to a particular Prefect server that is already running, then in your local terminal set <code>PREFECT_API_URL</code> to your desired server address:</p> <pre><code>prefect  config  set  PREFECT_API_URL=\"http://your-prefect-server/\"\nexport PREFECT_API_KEY=\"your-prefect-key\"\n</code></pre> <p>Otherwise, you can start a local server by running:</p> <pre><code>prefect server start\n</code></pre>"},{"location":"alcf832/#9-run-the-script-from-the-terminal","title":"9. Run the script from the terminal","text":"<pre><code>python orchestration/flows/bl832/alcf.py\n</code></pre> <p>Monitor the logs in the terminal or the Prefect UI, which will update you on the current status. - Step 1: Transfer data from data832 to ALCF - Step 2: Schedule Globus Compute Flows on ALCF     - A: Run <code>reconstruction.py</code>     - B: Run <code>tiff_to_zarr.py</code> - Step 3: Transfer reconstruction to data832</p> <p>Errors at step 1 or 3? - It may be authentication issues with your transfer client. Did you set <code>GLOBUS_CLIENT_ID</code> and <code>GLOBUS_CLIENT_SECRET</code> in <code>.env</code>? - It could also be permissions errors. Did you make a new service client? You will need to request permissions for the endpoints you are transferring to and from (both ALCF and NERSC).</p> <p>Errors at step 2? - It may be that your compute endpoint stopped!</p> <pre><code>- You can check on Polaris using:\n\n        globus-compute-endpoint list\n\n- If your endpoint is listed with the status \"Stopped,\" you can restart using:\n\n        globus-compute-endpoint start &lt; your endpoint name &gt;\n</code></pre> <ul> <li> <p>It may be authentication issues with your confidential client. Did you set the environment variables in your terminal on Polaris?</p> <p><code>bash export GLOBUS_COMPUTE_CLIENT_ID=\"your-client-id\" export GLOBUS_COMPUTE_CLIENT_SECRET=\"your-client-secret\"</code></p> <p>You can also check if you are logged into the confidential client: <code>bash globus-compute-endpoint whoami</code></p> </li> <li> <p>Check if the flow completed successfully</p> <p>If the data successfully transferred back to data832, you will find it in the following location:</p> <pre><code>/data/scratch/globus_share/&lt; folder name convention&gt;/rec&lt; dataset&gt;/&lt; tiffs&gt;\n</code></pre> </li> </ul>"},{"location":"alcf832/#schedule-pruning-with-prefect-workers","title":"Schedule Pruning with Prefect Workers","text":"<p>Following the previous steps to set up the Polaris and local environments enables <code>alcf.py</code> to properly run reconstruction and transfer data, however, additional steps are necessary for the file pruning to be scheduled and executed. This relies on Prefect Flows, Deployments, Work Pools, and Queues.</p>"},{"location":"alcf832/#terminology","title":"Terminology","text":"<ul> <li>Flow: Define a Prefect Flow in your code using the <code>@flow(name=\"flow_name\")</code> decorator. A flow encapsulates a series of tasks to be executed.</li> <li>Deployment: Deploy your Flow to your Prefect server, and assign it a work pool and queue. When called, deployments will create a flow run and submit it to a specific work pool for scheduling.</li> <li>Work Pool: Organizes workers depending on infrastructure implementation. Work pools manage how work is distributed among workers. For more information, check the documentation: https://docs.prefect.io/latest/concepts/work-pools/</li> <li>Queue: Each work pool has a \"default\" queue where work will be sent, and multiple queues can be added for fine-grained control over specific processes (i.e. setting priority and concurrency for different pruning functions).</li> <li>Workers: Formerly known as Agents. Can be assigned a specific Work Pool and Queue, and will listen to incoming jobs from those channels.</li> </ul>"},{"location":"alcf832/#pruning-example","title":"Pruning Example","text":"<p>This is an example of a pruning flow found in <code>orchestration/flows/bl832/prune.py</code>:</p> <pre><code>@flow(name=\"prune_alcf832_raw\")\ndef  prune_alcf832_raw(relative_path:  str):\n\n    p_logger = get_run_logger()\n    tc = initialize_transfer_client()\n    config = Config832()\n    globus_settings = JSON.load(\"globus-settings\").value\n    max_wait_seconds = globus_settings[\"max_wait_seconds\"]\n    p_logger.info(f\"Pruning {relative_path} from {config.alcf832_raw}\")\n    prune_one_safe(\n        file=relative_path,\n        if_older_than_days=0,\n        tranfer_client=tc,\n        source_endpoint=config.alcf832_raw,\n        check_endpoint=config.nersc832_alsdev_raw,\n        logger=p_logger,\n        max_wait_seconds=max_wait_seconds,)\n</code></pre> <p>To schedule and execute this flow from <code>alcf.py</code>, run this shell script in your terminal:</p> <pre><code>./create_deployments_832_alcf.sh\n</code></pre> <p>This script builds Prefect deployments for the different applicable prune functions in  <code>prune.py</code>, for example:</p> <pre><code># create_deployments_832_alcf.sh\nprefect  deployment  build  ./orchestration/flows/bl832/prune.py:prune_alcf832_raw  -n  prune_alcf832_raw  -q  bl832  -p  alcf_prune_pool\nprefect  deployment  apply  prune_alcf832_raw-deployment.yaml\n</code></pre> <p>Note: By default the deployments will not have access to the contents in the <code>.env</code> file, meaning the Globus confidential client ID and secret must be added using a different method. Instead, we can use Prefect Secret Blocks in the web UI to store  <code>GLOBUS_CLIENT_ID</code> and <code>GLOBUS_CLIENT_SECRET</code>, and securely import those into our script:</p> <pre><code>from prefect.blocks.system import Secret\n\nCLIENT_ID = Secret.load(\"globus-client-id\")\nCLIENT_SECRET = Secret.load(\"globus-client-secret\")\n# Access the stored secret\nsecret_block.get()\n</code></pre>"},{"location":"alcf832/#start-a-worker","title":"Start a worker","text":"<p>In a new terminal window, you can start a new worker by executing the following command:</p> <pre><code>prefect worker start -p alcf_prune_pool -q bl832\n</code></pre>"},{"location":"alcf832/#prefect-blocks","title":"Prefect Blocks","text":"<p>In the Prefect UI, navigate to the left column under \"Configuration\" and select \"Blocks.\" Press the \"+\" button to add a new Block from the catalog. Search for \"JSON\" and create a new JSON Block called \"pruning-config.\" This stores configuration information in a way that can be changed in the UI without adjusting the source code. For example:</p> <pre><code># pruning-config\n{\n    \"delete_alcf832_files_after_days\":2,\n    \"delete_data832_files_after_days\":3,\n    \"delete_nersc832_files_after_days\":7,\n    \"max_wait_seconds\":120\n}\n</code></pre> <p>Read more about Blocks here: https://docs.prefect.io/latest/concepts/blocks/</p>"},{"location":"alcf832/#helper-scripts","title":"Helper Scripts","text":"<p>We also provide several scripts for registering a new Globus Compute Flow, checking that the Globus Compute Endpoint is available, and ensuring that Globus Transfer has the correct permissions for reading, writing, and deleting data at a given transfer endpoint.</p>"},{"location":"alcf832/#check-globus-compute-status-orchestrationscriptscheck_globus_computepy","title":"Check Globus Compute Status <code>orchestration/scripts/check_globus_compute.py</code>","text":"<p>Example usage (from the command line):</p> <pre><code>python check_globus_compute.py --endpoint_id \"your-uuid-here\"\n</code></pre> <p>IMPORTANT. Ensure you are logged into Globus Compute in your local environment:</p> <pre><code>export GLOBUS_COMPUTE_CLIENT_ID=\"your-client-id\"\nexport GLOBUS_COMPUTE_CLIENT_SECRET=\"your-client-secret\"\n</code></pre>"},{"location":"alcf832/#check-globus-transfer-orchestrationscriptscheck_globus_transferpy","title":"Check Globus Transfer <code>orchestration/scripts/check_globus_transfer.py</code>","text":"<p>Run from the command line:</p> <pre><code>python check_globus_transfer.py --endpoint_id \"your-endpoint-id\"\n</code></pre> <p>IMPORTANT. Ensure you are logged into Globus Transfer in your local environment:</p> <pre><code>export GLOBUS_CLIENT_ID=\"your-client-id\"\nexport GLOBUS_CLIENT_SECRET=\"your-client-secret\"\n</code></pre>"},{"location":"alcf832/#login-to-globus-and-prefect-orchestrationscriptslogin_to_globus_and_prefectsh","title":"Login to Globus and Prefect <code>orchestration/scripts/login_to_globus_and_prefect.sh</code>","text":"<p>Run this script to log into Globus Transfer, Globus Compute, and Prefect simultaneously (assuming you already filled your <code>.env</code> file with the correct login information). This can speed up logging in for your sessions.</p> <p>Example usage:</p> <pre><code>source ./login_to_globus_and_prefect.sh\n</code></pre>"},{"location":"configuration/","title":"Configuration","text":""},{"location":"configuration/#summary-of-configyaml","title":"Summary of <code>config.yaml</code>","text":"<p>The <code>config.yaml</code> file contains configurations for various components involved in the data management, processing, and orchestration workflows related to the ALS beamlines.</p>"},{"location":"configuration/#globus-endpoints","title":"Globus Endpoints","text":"<ul> <li>globus_endpoints: Defines multiple Globus endpoints used for data transfer between various systems.</li> <li>spot832, data832, alcf832, nersc832, etc., each has specific configurations like:<ul> <li><code>root_path</code>: The root directory for data storage.</li> <li><code>uri</code>: The URI for accessing the endpoint.</li> <li><code>uuid</code>: Unique identifier for each endpoint.</li> <li><code>name</code>: Descriptive name for the endpoint.</li> </ul> </li> <li>These endpoints represent different storage locations across systems like ALS, ALCF, and NERSC.</li> </ul>"},{"location":"configuration/#globus-apps","title":"Globus Apps","text":"<ul> <li>globus_apps: Defines the application configurations needed for Globus transfers, such as the <code>als_transfer</code> app, with environment variables for <code>client_id</code> and <code>client_secret</code>.</li> </ul>"},{"location":"configuration/#harbor-images","title":"Harbor Images","text":"<ul> <li>harbor_images832: Specifies the image for tomography reconstruction (<code>recon_image</code>) and multi-resolution conversion (<code>multires_image</code>) in the Harbor registry.</li> </ul>"},{"location":"configuration/#ghcr-images","title":"GHCR Images","text":"<ul> <li>ghcr_images832: Specifies the same tomography images as above but stored in the GitHub Container Registry (GHCR) for easier access during workflows.</li> </ul>"},{"location":"configuration/#prefect","title":"Prefect","text":"<ul> <li>prefect.deployments: Defines the Prefect flow deployments, including unique identifiers (<code>uuid</code>) for specific flows related to file processing, like <code>new_file_832</code>.</li> </ul>"},{"location":"configuration/#scicat","title":"SciCat","text":"<ul> <li>scicat: Configures the SciCat metadata service used for ingesting jobs and managing metadata. It includes the <code>jobs_api_url</code> for the API used to track and ingest jobs into the SciCat platform.</li> </ul>"},{"location":"configuration/#key-configuration-highlights","title":"Key Configuration Highlights:","text":"<ul> <li>The YAML defines detailed configurations for each endpoint, ensuring smooth integration between systems like Globus, ALCF, NERSC, and data832.</li> <li>Provides details for image containers used in tomography processes and how to access them.</li> <li>Specifies the integration of SciCat for metadata tracking and the Prefect orchestration system for scheduling workflows.</li> </ul>"},{"location":"getting_started/","title":"Getting Started","text":"<p>To use the full functionality of this project, there are a few additional steps required to get started. Some of our workflows are run at specific HPC facilities, which requires getting user accounts and project access. We also rely on Prefect, a data orchestration tool, to launch and monitor our jobs.</p>"},{"location":"getting_started/#hpc-access","title":"HPC Access","text":"<p>We support running flows on a couple of DOE supercomputer facilities, ALCF and NERSC, and in the near future we will support OLCF as well. Since each facility has different hardware and software configurations, the steps for gaining access to and running code on these systems will vary.</p>"},{"location":"getting_started/#alcf","title":"ALCF","text":"<p>We support running computation tasks at Argonne Leadership Compute Facility (ALCF) through Globus Compute. This webinar provides a great overview to running remote workflows at ALCF and the tools that are available. To gain access to this system, you can request an account.</p> <p>Once you have an account, you can login to Polaris:</p> <p>SSH into Polaris</p> <p>Login to Polaris using the following terminal command:</p> <pre><code>ssh &lt;your ALCF username&gt;@polaris.alcf.anl.gov\n</code></pre> <p>Password: copy passcode from the MobilePASS+ app.</p>"},{"location":"getting_started/#nersc","title":"NERSC","text":"<p>We schedule computing tasks at National Energy Research Scientific Computing Center (NERSC) by leveraging Docker, SLURM, and SFAPI. To get access to these systems follow these instructions.</p> <p>Once you have an account, you can follow along here to see how to schedule remote tomography reconstructions at NERSC.</p>"},{"location":"getting_started/#olcf","title":"OLCF","text":"<ul> <li> <p>Request access and login to OLCF.</p> </li> <li> <p>OLCF user documentation.</p> </li> </ul>"},{"location":"getting_started/#globus","title":"Globus","text":"<p>We use Globus for two main purposes:</p> <ol> <li>High-speed file transfer</li> <li>Scheduling and running compute jobs</li> </ol> <p>Follow these instructions to get started with Globus.</p>"},{"location":"getting_started/#confidential-client","title":"Confidential Client","text":"<p>Once you have a Globus account, you can create a <code>Confidential Client</code>. This generates a unique ID and key for authenticating with Globus through the API, avoiding a manual authentication step redirecting to the web login page. </p> <p>Use an existing Globus Confidential Client, or create a new one</p> <ul> <li>In your browser, navigate to globus.org</li> <li>Login</li> <li>On the left, navigate to \"Settings\"</li> <li>On the top navigation bar, select \"Developers\" </li> <li>On the right under Projects, create a new project called Splash Flows or select an existing one</li> <li>Create a new registered app called Splash Flows App, or select an existing one</li> <li>Generate a secret</li> <li>Store the confidential client UUID and Secret (note: make sure you copy the Client UUID and not the Secret UUID)</li> </ul> <p>Note: For each service client, you will need to get permissions set by the correct person at ALCF, NERSC or other facility to be able to use it for transfer endpoints depending on how guest collections are configured.</p>"},{"location":"getting_started/#globus-transfer","title":"Globus Transfer","text":"<p>Globus allows us to move data between computers at the ALS, and also to transfer endpoints at HPC facilities. Transferring data via Globus can be done in their web user interface, as well as in Python via their API.</p>"},{"location":"getting_started/#globus-compute","title":"Globus Compute","text":"<p>For some data workflows we use Globus Compute to easily execute our code at an HPC facility such as ALCF. This requires configuring a globus-compute-endpoint on the HPC system that listens for incoming compute tasks and schedules those jobs as soon as resources are available. \u00a0</p> <p>Learn how to get Globus Compute set up at ALCF to run tomography reconstructions.</p>"},{"location":"getting_started/#prefect","title":"Prefect","text":"<p>Prefect is a workflow orchestration tool that allows us to register Python functions as \"Tasks\" and \"Flows\" that can be called, scheduled, and monitored nicely within their web user interface.</p> <p>There are several approaches to using Prefect. </p> <ol> <li>Prefect Cloud<ul> <li>Good for local development and debugging.</li> <li>Less ideal for a production environment.</li> </ul> </li> <li>Your own Prefect server deployment<ul> <li>Scalable, but you are responsible for making sure the server is running.</li> <li>Ideal for production environments where you are launching Docker containers.</li> </ul> </li> </ol> <p>Here is an example of the service_configs for our Prefect server.</p>"},{"location":"getting_started/#login-to-prefect-in-your-environment","title":"Login to Prefect in your environment","text":"<p>Before running or registering any flows, you must ensure you are authenticated with your Prefect server. Depending on your setup, do one of the following:</p> <ul> <li> <p>For Prefect Cloud:   Run the following command and follow the instructions to authenticate:</p> <p><code>bash prefect cloud login</code></p> <p>or you can run the following to set your env variables directly:</p> <p><code>prefect config set PREFECT_API_URL=\"[API-URL]\"</code></p> <p><code>prefect config set PREFECT_API_KEY=\"[API-KEY]\"</code></p> </li> <li> <p>For self-hosted Prefect server:</p> <p>Please refer to the official documentation.</p> </li> </ul>"},{"location":"getting_started/#register-a-flow","title":"Register a Flow","text":"<p>You can run the following scripts to register the Prefect Flows in this project to the Prefect server configured in your environment.</p> <ul> <li> <p><code>./create_deployment_832_dispatcher.sh</code></p> </li> <li> <p><code>./create_deployments_7012.sh</code></p> </li> <li> <p><code>./create_deployments_832.sh</code></p> </li> <li> <p><code>./create_deployments_832_alcf.sh</code></p> </li> <li> <p><code>./create_deployments_832_nersc.sh</code></p> </li> </ul>"},{"location":"getting_started/#start-a-agentsworkers","title":"Start a Agents/Workers","text":"<p>Prefect Agents are defined in the <code>create_deployments.sh</code> scripts, and can be started like this:</p> <pre><code>prefect agent start --pool \"alcf_flow_pool\"\n</code></pre> <p>This creates an agent that listens to jobs on the <code>alcf_flow_pool</code> \"Work Pool\". In this way, each one of our deployed Prefect Flows can be associated with a specific Work Pool, allowing fine-grained control over the system.</p>"},{"location":"getting_started/#call-a-flow","title":"Call a Flow","text":"<p>Once a flow is registered as a deployment and your agent is running, you can trigger a run in several ways:</p> <ul> <li> <p>Via the Prefect UI</p> <p>Navigate to your Prefect server dashboard, locate the registered flow deployment, and trigger a run manually.</p> </li> <li> <p>Via the Command Line Interface</p> <p>You can also trigger a run using the Prefect CLI:</p> <pre><code>prefect deployment run &lt;FLOW_NAME&gt;/&lt;DEPLOYMENT_NAME&gt;\n</code></pre> <p>Replace <code>&lt;FLOW_NAME&gt;/&lt;DEPLOYMENT_NAME&gt;</code> with the name of the deployment you want to run (e.g., <code>alcf_recon_flow/alcf_recon_flow</code>).</p> </li> <li> <p>Via API Calls</p> <p>For automation purposes, you may also call the Prefect API from your own scripts or applications using the Python SDK</p> <pre><code>run_deployment(\n    name=\"my-first-flow/my-first-deployment\",\n    parameters={\"my_param\": \"42\"},\n    job_variables={\"env\": {\"MY_ENV_VAR\": \"staging\"}},\n    timeout=0, # don't wait for the run to finish\n)\n</code></pre> </li> </ul>"},{"location":"glossary/","title":"Glossary","text":""},{"location":"glossary/#alcf","title":"ALCF","text":"<p>ALCF: The Argonne Leadership Computing Facility (ALCF) provides advanced computational resources for large-scale scientific research, offering systems like Aurora and Theta for high-performance computing.</p>"},{"location":"glossary/#docker","title":"Docker","text":"<p>Docker: A platform for developing, shipping, and running applications in lightweight, portable containers. Docker containers isolate applications and their dependencies, making them easy to deploy and scale across different environments.</p> <ul> <li>Dockerfile: A text file that contains a series of instructions on how to build a Docker image. Each instruction in a Dockerfile creates a layer in the image, defining the environment and the application to be run. Dockerfiles are used to automate the creation of Docker images, ensuring consistency and reproducibility across different environments.</li> <li>Image: A Docker image is a read-only template that contains the instructions and filesystem layers needed to create a container. It\u2019s like a blueprint. Images are immutable (cannot be changed once created), contain operating system dependencies, application code, libraries, environment variables, and metadata like default commands (via CMD or ENTRYPOINT). They can be built (via a Dockerfile), pulled from a registry (e.g., Docker Hub, private repositories), and shared between environments.</li> <li>Container: A Docker container is a runtime instance of a Docker image. Once the image is instantiated (started), it becomes a container. Containers are mutable at runtime (you can create/write files, run processes, etc.), and while they are isolated, they can interact with the host system (networking, mounts, etc.) as configured. Each container has its unique Container ID, its own filesystem layers (based on the image), and any ephemeral changes are lost unless committed as a new image or preserved using volumes.</li> </ul>"},{"location":"glossary/#flake8","title":"flake8","text":"<p>flake8: A Python tool for enforcing coding standards and style. Flake8 checks Python code for PEP 8 compliance, potential errors, and other common issues that could affect code quality and readability.</p>"},{"location":"glossary/#globus","title":"Globus","text":"<p>Globus is a platform offering services for secure data transfer and remote compute across distributed systems.</p> <ul> <li>Compute: A service provided by Globus that allows users to execute workflows and processes on remote computing resources. It is often used for running large-scale jobs on distributed systems such as HPC facilities.</li> <li>Transfer: The service in Globus that enables the movement of data between endpoints. It handles secure, high-performance file transfer between various locations on the network.</li> <li>Confidential Client: A type of client used in Globus authentication systems. Confidential clients are trusted applications that can securely store credentials, unlike public clients that rely on user consent for access.</li> <li>globus-compute-endpoint: An endpoint used by Globus Compute to manage and execute remote computing jobs. These endpoints are configured to run workflows, tasks, and pipelines on distributed computing resources.</li> </ul>"},{"location":"glossary/#hpc","title":"HPC","text":"<p>HPC (High Performance Computing): High Performance Computing (HPC) refers to the use of advanced computational resources, including supercomputers and computer clusters, to solve complex and computationally intensive problems. HPC systems enable parallel processing and high-speed calculations that are essential for research and development in scientific, engineering, and industrial fields. These systems help tackle large-scale simulations, data analyses, and modeling tasks that cannot be efficiently managed by standard computing setups.</p>"},{"location":"glossary/#itk-vtk-viewer","title":"itk-vtk-viewer","text":"<p>itk-vtk-viewer: A visualization tool that integrates ITK (Insight Segmentation and Registration Toolkit) and VTK (Visualization Toolkit) to view 3D image data and volume renderings. It is commonly used for scientific and medical image processing.</p>"},{"location":"glossary/#nersc","title":"NERSC","text":"<p>NERSC: The National Energy Research Scientific Computing Center (NERSC) is a major supercomputing center for the U.S. Department of Energy\u2019s Office of Science. NERSC supports a variety of scientific research projects and provides computational resources such as Cori and Perlmutter.</p>"},{"location":"glossary/#olcf","title":"OLCF","text":"<p>OLCF: The Oak Ridge Leadership Computing Facility (OLCF) provides some of the world's most powerful supercomputers to support scientific research. It includes systems such as Summit and Frontier, which enable simulations and modeling on a large scale.</p>"},{"location":"glossary/#prefect","title":"Prefect","text":"<p>Prefect is an orchestration framework for modern data workflows.</p> <ul> <li>Agent: A component of Prefect that listens for tasks and flows to be scheduled for execution. Agents are responsible for running and managing the tasks in a Prefect deployment.</li> <li>Flow: A collection of tasks in Prefect that define a workflow. Flows are the central part of Prefect's orchestration, encapsulating task dependencies, execution logic, and data flow.</li> <li>Task: An individual unit of work within a Prefect flow. Tasks can be executed in sequence or in parallel, depending on the dependencies defined in the flow.</li> <li>Server: The centralized control system in Prefect that manages flows, schedules, and task execution. The Prefect server stores metadata, logs, and state information for all flows and tasks.</li> <li>Blocks: A feature in Prefect that enables reusable components or functions within flows. Blocks can represent things like storage systems, APIs, or other tools that need to be configured and reused within different flows.</li> <li>Deployments: A deployment in Prefect refers to the configuration and setup of a flow to run in a specific environment. It includes details about the flow's parameters, the execution context, and deployment target.</li> </ul>"},{"location":"glossary/#scicat","title":"SciCat","text":"<p>SciCat: A scientific data management system designed for use in large research organizations and facilities. SciCat helps researchers store, organize, and track experimental data, metadata, and related information in a structured and accessible way.</p>"},{"location":"glossary/#sfapi","title":"SFAPI","text":"<p>SFAPI: NERSC Superfacility Application Programming Interface (API). The API can be used to script jobs or workflows running against NERSC systems. The goal is to support everything from a simple script that submits a job to complex workflows that move files, check job progress, and make advanced compute time reservations.</p>"},{"location":"glossary/#pytest","title":"pytest","text":"<p>pytest: A framework for testing Python code, providing an easy-to-use interface for writing and running tests. Pytest supports a wide range of testing capabilities, including unit tests, integration tests, and fixture management.</p>"},{"location":"glossary/#tiled","title":"Tiled","text":"<p>Tiled: A Python package for managing large, multidimensional datasets (such as images or scientific data). Tiled provides an efficient way to access and process data in chunks, optimizing memory usage and performance.</p>"},{"location":"install/","title":"Installation","text":"<p>Splash Flows Globus contains configuration, code and infrastructure for moving data and running automated computing tasks when X-ray scans are captured at the Advanced Light Source.</p> <p>This package is primarily written in Python, and relies on additional services such as Prefect (an orchestration tool), Docker (for containers), and Globus (for Transfer and Compute).</p> <p>These flows can be run from the command line, or built into a self-contained docker container, which can be found under Releases.\u00a0</p>"},{"location":"install/#getting-started","title":"Getting started","text":""},{"location":"install/#clone-this-repo","title":"Clone this repo","text":"<pre><code>$   git clone git@github.com:als-computing/splash_flows_globus.git\n$   cd splash_flows_globus\n</code></pre> <p>It is recommended to create a new conda environment when pulling the source code locally, so you do not end up with conflicts with your global environment. Conda is an OS-agnostic, system-level binary package and environment manager, and installation information can be found here.</p> <p>Once you have conda on your system, you can create and activate a new environment like this:</p> <pre><code>conda create -n splash_flows_globus_env python=3.11.9\nconda activate splash_flows_globus_env\n</code></pre> <p>Now that you have activated your environment, you can install the necessary Python dependencies by calling the following line in the root directory of the <code>splash_flows_globus</code> project:</p> <pre><code>pip install -r reqirements.txt\n</code></pre> <p>Now that the requirements are installed, you can install the package:</p> <pre><code>pip install -e .\n</code></pre>"},{"location":"nersc832/","title":"Run Tomography Reconstruction Remotely at NERSC","text":""},{"location":"nersc832/#request-an-account","title":"Request an account","text":"<p>Refer to the NERSC Documentation on how to obtain a user account.</p>"},{"location":"nersc832/#login-to-iris","title":"Login to Iris","text":"<p>NERSC user accounts are managed in the Iris system, which you can access once you have an account. For more information on how to use Iris, please see the Iris documentation.</p>"},{"location":"nersc832/#sfapi","title":"SFAPI","text":"<p>Superfacility API Clients (SFAPI) is the gateway to running remote tasks on NERSC, and the method we support for scheduling jobs via SLURM. To create a new SFAPI client:</p> <ol> <li>Login to Iris and navigate to the Profile section, found either on the menu bar at the top or under Settings on the left panel.</li> <li>Scroll all the way to the bottom to the Superfacility API Clients section.</li> <li> <p>Press the '+ New Client' button     Fill out the form</p> <p>a. Client Name (ex: 'tomo-sfapi')</p> <p>b. Comments (blank)</p> <p>c. User to create client for ('alsdev')</p> <p>d. Which security level does your client need ('Red')</p> <p>e. IP address ranges (Your IP, perlmutter nodes)</p> </li> <li> <p>Save the SFAPI keys in a safe place. You will need these to launch jobs on NERSC.</p> </li> </ol>"},{"location":"nersc832/#nersc-system-status","title":"NERSC System Status","text":"<p>Sometimes the system is down. Check the status for unexpected and routine maintainence.</p>"},{"location":"nersc832/#slurm","title":"Slurm","text":"<p>NERSC uses the Slurm workload manager to schedule jobs. Here is an example of Slurm job script we use to run reconstruction:</p> <pre><code>#!/bin/bash\n#SBATCH -q realtime\n#SBATCH -A als\n#SBATCH -C cpu\n#SBATCH --job-name=tomo_multires_{folder_name}_{file_name}\n#SBATCH --output={pscratch_path}/tomo_recon_logs/%x_%j.out\n#SBATCH --error={pscratch_path}/tomo_recon_logs/%x_%j.err\n#SBATCH -N 1\n#SBATCH --ntasks-per-node 1\n#SBATCH --cpus-per-task 64\n#SBATCH --time=0:15:00\n#SBATCH --exclusive\n\ndate\n\necho \"Running multires container...\"\nsrun podman-hpc run \\\n--volume {recon_scripts_dir}/tiff_to_zarr.py:/alsuser/tiff_to_zarr.py \\\n--volume {pscratch_path}/8.3.2:/alsdata \\\n--volume {pscratch_path}/8.3.2:/alsuser/ \\\n{multires_image} \\\nbash -c \"python tiff_to_zarr.py {recon_path} --raw_file {raw_path}\"\n\ndate\n</code></pre> <p>Definitions:</p> <ul> <li>--q: the selected queue or QOS<ul> <li>NERSC provides a few different queue options. The <code>debug</code> queue is usually quick to pick up jobs for testing purposes.</li> <li>The <code>alsdev</code> account on NERSC has access to the <code>realtime</code> queue. You must request access to use this queue.</li> </ul> </li> <li> <p>--A: the project on NERSC to charge the job</p> </li> <li> <p>--job-name: Sets a descriptive name for the job  </p> <ul> <li>This name (using variables such as <code>{folder_name}</code> and <code>{file_name}</code>) appears in job listings and log files, making it easier to identify and manage jobs.</li> </ul> </li> <li> <p>--output: Specifies the file path where the job's standard output (stdout) will be written  </p> <ul> <li>The placeholders <code>%x</code> (job name) and <code>%j</code> (job ID) uniquely name the log file for each job.</li> </ul> </li> <li> <p>--error: Specifies the file path where the job's standard error (stderr) will be written  </p> <ul> <li>Similar to <code>--output</code>, <code>%x</code> and <code>%j</code> ensure error log files are uniquely named based on the job name and job ID.</li> </ul> </li> <li> <p>-N: Sets the number of compute nodes to allocate  </p> <ul> <li><code>-N 1</code> indicates the job will run on a single node.</li> </ul> </li> <li> <p>--ntasks-per-node: Defines how many tasks run on each node  </p> <ul> <li><code>--ntasks-per-node 1</code> runs one task per node, often used with multi-threading in a single task.</li> </ul> </li> <li> <p>--cpus-per-task: Number of CPUs allocated to each task  </p> <ul> <li>In this example, each task gets 64 CPUs, suitable for multi-threaded workloads.</li> </ul> </li> <li> <p>--time: Maximum wall clock time allowed for the job  </p> <ul> <li><code>0:15:00</code> means a 15-minute time limit before the job is terminated if it exceeds this duration.</li> </ul> </li> <li> <p>--exclusive: Reserves the allocated node(s) exclusively for this job  </p> <ul> <li>Prevents other jobs from sharing the same node resources.</li> </ul> </li> </ul>"},{"location":"nersc832/#deploy-this-flow-to-prefect","title":"Deploy this flow to Prefect","text":"<p>The script <code>orchestration/flows/bl832/nersc.py</code> contains the logic to run the reconstruction and multi-resolution jobs at NERSC via SFAPI, the Slurm script, and Globus Transfers.</p> <p>To create agents and deployments for this code, you can run:</p> <pre><code>./create_deployments_832_nersc.sh\n</code></pre> <p>And then in your terminal, start up the Prefect agents:</p> <pre><code>prefect agent start --pool \"nersc_flow_pool\"\nprefect agent start --pool \"nersc_prune_pool\"\n</code></pre> <p>This starts two agents assigned to two different Work Pools: one for the main reconstruction flow, and another to handle file pruning commands that are scheduled in the future.</p>"},{"location":"nersc832/#help-tickets","title":"Help tickets","text":"<p>Submit a NERSC ticket.</p>"},{"location":"orchestration/","title":"Orchestration","text":"<p>A common architecture for moving data is defined in <code>orchestration/transfer_controller.py</code>.</p>"},{"location":"orchestration/#bl7012","title":"bl7012","text":"<p>Info goes here</p>"},{"location":"orchestration/#bl733","title":"bl733","text":"<p>Info goes here</p>"},{"location":"orchestration/#bl832","title":"bl832","text":"<p>Beamline 8.3.2 is the Hard X-ray Micro-tomography instrument at the Advanced Light Source.</p>"},{"location":"orchestration/#configpy","title":"<code>config.py</code>","text":"<p>The <code>Config832</code> class is designed to configure and initialize various components needed for data transfer and orchestration workflows related to the ALS beamline 832.</p> <ul> <li> <p>Initialization:     The <code>Config832</code> class retrieves configuration data using the <code>transfer.get_config()</code> function, which is used to set up necessary endpoints and applications for data transfer.</p> </li> <li> <p>Endpoints and Applications:     It constructs a set of endpoints using <code>transfer.build_endpoints()</code> and applications with <code>transfer.build_apps()</code>. These endpoints represent different storage locations for data, both on local and remote systems (e.g., <code>spot832</code>, <code>data832</code>, <code>nersc832</code>, etc.).</p> </li> <li> <p>Transfer Client:     A <code>TransferClient</code> instance is created using <code>transfer.init_transfer_client()</code>, with a specified application (<code>als_transfer</code>). This client will handle the file transfer operations between different endpoints.</p> </li> <li> <p>Storage Locations:     Multiple endpoints related to data storage are stored as attributes, representing raw data, scratch space, and other storage locations for different systems (e.g., <code>data832_raw</code>, <code>nersc832_alsdev_scratch</code>).</p> </li> <li> <p>Additional Configurations:     The configuration dictionary (<code>config</code>) also contains other configuration values like <code>scicat</code> (for metadata) and <code>ghcr_images832</code> (for container images), which are also stored as attributes.</p> </li> </ul>"},{"location":"orchestration/#dispatcherpy","title":"<code>dispatcher.py</code>","text":"<p>This script is designed to automate and orchestrate the decision-making process for the BL832 beamline, ensuring that the appropriate workflows are triggered based on predefined settings.</p>"},{"location":"orchestration/#key-components","title":"Key Components:","text":"<ul> <li> <p><code>FlowParameterMapper</code> Class:     This class is used to map the required parameters for each flow based on a predefined set of parameters. The <code>get_flow_parameters</code> method filters and returns only the relevant parameters for the specified flow based on a dictionary of available parameters.</p> </li> <li> <p><code>DecisionFlowInputModel</code>:     A Pydantic model that validates input parameters for the decision flow, including the file path, export control status, and configuration dictionary. It ensures that the input parameters are in the correct format before they are used in the decision-making process.</p> </li> <li> <p><code>setup_decision_settings</code> Task:     This task defines the settings for which flows should be executed (e.g., ALCF, NERSC, and 832 beamline file management). It logs the configuration and saves the decision settings as a JSON block for later use by other flows.</p> </li> <li> <p><code>run_specific_flow</code> Task:     An asynchronous task that runs a specific flow based on the provided flow name and parameters. This task is dynamically executed and uses the Prefect deployment system to trigger flows as needed.</p> </li> <li> <p><code>dispatcher</code> Flow:     The main flow, which coordinates the execution of various tasks:  </p> <ol> <li>Validates input parameters using <code>DecisionFlowInputModel</code>.  </li> <li>Loads decision settings from a previously saved JSON block.  </li> <li>Runs the <code>new_832_file_flow/new_file_832</code> flow synchronously first if required.  </li> <li>Runs ALCF and NERSC flows asynchronously if they are enabled in the decision settings. This flow ensures that the correct sub-flows are executed in the correct order, using dynamic parameters for each flow.</li> </ol> </li> </ul>"},{"location":"orchestration/#movepy","title":"<code>move.py</code>","text":"<p>This script defines a set of tasks and flows for transferring files between different endpoints and systems, processing data, and managing file deletions. It leverages Globus for file transfers and Prefect for orchestration.</p>"},{"location":"orchestration/#key-components_1","title":"Key Components:","text":"<ol> <li> <p><code>transfer_spot_to_data</code> Task:  </p> <ul> <li>This task transfers a file from the <code>spot832</code> endpoint to the <code>data832</code> endpoint using the <code>GlobusTransferClient</code>.  </li> <li>It ensures that the file paths are correctly formatted and calls the <code>start_transfer</code> function to perform the transfer.</li> </ul> </li> <li> <p><code>transfer_data_to_nersc</code> Task:  </p> <ul> <li>Similar to the previous task, this one transfers data from <code>data832</code> to the <code>nersc832</code> endpoint.  </li> <li>The task also handles file path formatting and logs transfer details.</li> </ul> </li> <li> <p><code>process_new_832_file</code> Flow:  </p> <ul> <li>This is the main flow for processing new files in the system. It performs multiple steps:  <ol> <li>Transfers the file from <code>spot832</code> to <code>data832</code> using <code>transfer_spot_to_data</code>.  </li> <li>Transfers the file from <code>data832</code> to <code>NERSC</code> if export control is not enabled and <code>send_to_nersc</code> is true, using <code>transfer_data_to_nersc</code>.  </li> <li>Ingests the file into SciCat using the <code>ingest_dataset</code> function. If SciCat ingestion fails, it logs an error.  </li> <li>Schedules file deletion tasks for both <code>spot832</code> and <code>data832</code> using the <code>schedule_prefect_flow</code> function, ensuring files are deleted after a set number of days as configured in <code>bl832-settings</code>.</li> </ol> </li> </ul> </li> <li> <p><code>test_transfers_832</code> Flow:  </p> <ul> <li>This flow is used for testing transfers between <code>spot832</code>, <code>data832</code>, and <code>NERSC</code>. It:  <ol> <li>Generates a new unique file name.  </li> <li>Transfers the file from <code>spot832</code> to <code>data832</code> and then from <code>data832</code> to <code>NERSC</code>.  </li> <li>Logs the success of each transfer and checks that the process works as expected.</li> </ol> </li> </ul> </li> </ol>"},{"location":"orchestration/#configuration","title":"Configuration:","text":"<ul> <li><code>API_KEY</code>: The API key is retrieved from the environment variable <code>API_KEY</code>, which is used for authorization with Globus and other services.</li> <li><code>TOMO_INGESTOR_MODULE</code>: This is a reference to the module used for ingesting datasets into SciCat.</li> <li><code>Config832</code>: The configuration for the beamline 832 environment, which includes the necessary endpoints for file transfers and other operations.</li> </ul>"},{"location":"orchestration/#file-deletion-scheduling","title":"File Deletion Scheduling:","text":"<ul> <li>After transferring the files, the flow schedules the deletion of files from <code>spot832</code> and <code>data832</code> after a predefined number of days, using Prefect's flow scheduling capabilities.</li> </ul>"},{"location":"orchestration/#error-handling","title":"Error Handling:","text":"<ul> <li>Throughout the script, errors are logged in case of failure during file transfers, SciCat ingestion, or scheduling tasks.</li> </ul>"},{"location":"orchestration/#use-case","title":"Use Case:","text":"<p>This script is primarily used for automating the movement of files from <code>spot832</code> to <code>data832</code>, sending data to <code>NERSC</code>, ingesting the data into SciCat, and managing file cleanup on both <code>spot832</code> and <code>data832</code> systems. It allows for flexible configuration based on export control settings and NERSC transfer preferences.</p>"},{"location":"orchestration/#job_controllerpy","title":"<code>job_controller.py</code>","text":"<p>This script defines an abstract class and factory function for managing tomography reconstruction and multi-resolution dataset building on different High-Performance Computing (HPC) systems. It uses the <code>Config832</code> class for configuration and handles different HPC environments like ALCF and NERSC. The abstraction allows for easy expansion to support additional systems like OLCF in the future.</p>"},{"location":"orchestration/#key-components_2","title":"Key Components:","text":"<ol> <li> <p><code>TomographyHPCController</code> Class (Abstract Base Class):</p> <ul> <li>This abstract base class defines the interface for tomography HPC controllers, with methods for performing tomography reconstruction and generating multi-resolution datasets. </li> <li>Methods:<ul> <li><code>reconstruct(file_path: str) -&gt; bool</code>:     Performs tomography reconstruction for a given file. Returns <code>True</code> if successful, <code>False</code> otherwise.</li> <li><code>build_multi_resolution(file_path: str) -&gt; bool</code>:     Generates a multi-resolution version of the reconstructed tomography data for a given file. Returns <code>True</code> if successful, <code>False</code> otherwise.</li> </ul> </li> </ul> </li> <li> <p><code>HPC</code> Enum:</p> <ul> <li>An enum to represent different HPC environments, with members:<ul> <li><code>ALCF</code>: Argonne Leadership Computing Facility</li> <li><code>NERSC</code>: National Energy Research Scientific Computing Center</li> <li><code>OLCF</code>: Oak Ridge Leadership Computing Facility (currently not implemented)</li> </ul> </li> </ul> </li> <li> <p><code>get_controller</code> Function:</p> <ul> <li>A factory function that returns an appropriate <code>TomographyHPCController</code> subclass instance based on the specified HPC environment (<code>ALCF</code>, <code>NERSC</code>, or <code>OLCF</code>).</li> <li>Parameters:<ul> <li><code>hpc_type</code>: An enum value identifying the HPC environment (e.g., <code>ALCF</code>, <code>NERSC</code>).</li> <li><code>config</code>: A <code>Config832</code> object containing configuration data.</li> </ul> </li> <li>Returns: An instance of the corresponding <code>TomographyHPCController</code> subclass.</li> <li>Raises: A <code>ValueError</code> if the <code>hpc_type</code> is invalid or unsupported, or if no config object is provided.</li> </ul> </li> </ol>"},{"location":"orchestration/#alcfpy","title":"<code>alcf.py</code>","text":"<p>This script is responsible for performing tomography reconstruction and multi-resolution data processing on ALCF using Globus Compute. It orchestrates file transfers, reconstructs tomography data, and builds multi-resolution datasets, then transfers the results back to the <code>data832</code> endpoint.</p>"},{"location":"orchestration/#key-components_3","title":"Key Components:","text":"<ol> <li> <p><code>ALCFTomographyHPCController</code> Class:</p> <ul> <li>This class implements the <code>TomographyHPCController</code> abstract class for the ALCF environment, enabling tomography reconstruction and multi-resolution dataset creation using Globus Compute.</li> <li>Methods:<ul> <li><code>reconstruct(file_path: str) -&gt; bool</code>:     Runs the tomography reconstruction using Globus Compute by submitting a job to ALCF.</li> <li><code>build_multi_resolution(file_path: str) -&gt; bool</code>:     Converts TIFF files to Zarr format using Globus Compute.</li> <li><code>_reconstruct_wrapper(...)</code>:     A static method that wraps the reconstruction process, running the <code>globus_reconstruction.py</code> script.</li> <li><code>_build_multi_resolution_wrapper(...)</code>:     A static method that wraps the TIFF to Zarr conversion, running the <code>tiff_to_zarr.py</code> script.</li> <li><code>_wait_for_globus_compute_future(future: Future, task_name: str, check_interval: int = 20) -&gt; bool</code>:     Waits for a Globus Compute task to complete and checks its status at regular intervals.</li> </ul> </li> </ul> </li> <li> <p><code>schedule_prune_task</code> Task:</p> <ul> <li>Schedules the deletion of files from a specified location, allowing the deletion of processed files from ALCF, NERSC, or data832.</li> <li>Parameters:  <ul> <li><code>path</code>: The file path to the folder containing files for deletion.  </li> <li><code>location</code>: The server location where the files are stored.  </li> <li><code>schedule_days</code>: The delay before deletion, in days.  </li> </ul> </li> <li>Returns: <code>True</code> if the task was scheduled successfully, <code>False</code> otherwise.</li> </ul> </li> <li> <p><code>schedule_pruning</code> Task:</p> <ul> <li>This task schedules multiple file pruning operations based on configuration settings. It takes paths for various scratch and raw data locations, including ALCF, NERSC, and data832.</li> <li>Parameters:  <ul> <li><code>alcf_raw_path</code>, <code>alcf_scratch_path_tiff</code>, <code>alcf_scratch_path_zarr</code>, etc.: Paths for various file locations to be pruned.  </li> <li><code>one_minute</code>: A flag for testing purposes, scheduling pruning after one minute.  </li> <li><code>config</code>: Configuration object for the flow.</li> </ul> </li> <li>Returns: <code>True</code> if all tasks were scheduled successfully, <code>False</code> otherwise.</li> </ul> </li> <li> <p><code>alcf_recon_flow</code> Flow:</p> <ul> <li> <p>This is the main flow for processing and transferring files between <code>data832</code> and ALCF. It orchestrates the following steps:</p> <ol> <li>Transfer data from <code>data832</code> to ALCF.</li> <li>Run tomography reconstruction on ALCF using Globus Compute.</li> <li>Run TIFF to Zarr conversion on ALCF using Globus Compute.</li> <li>Transfer the reconstructed data (both TIFF and Zarr) back to <code>data832</code>.</li> <li>Schedule pruning tasks to delete files from ALCF and data832 after the defined period.</li> </ol> </li> <li> <p>Parameters:  </p> <ul> <li><code>file_path</code>: The file to be processed, typically in <code>.h5</code> format.  </li> <li><code>config</code>: Configuration object containing the necessary endpoints for data transfers.</li> </ul> </li> <li> <p>Returns: <code>True</code> if the flow completed successfully, <code>False</code> otherwise.</p> </li> </ul> </li> </ol>"},{"location":"orchestration/#nerscpy","title":"<code>nersc.py</code>","text":"<p>This script manages tomography reconstruction and multi-resolution data processing on NERSC using the SFAPI client. It submits jobs for reconstruction and multi-resolution processing, transfers data between NERSC and data832, and schedules pruning tasks for file cleanup.</p>"},{"location":"orchestration/#key-components_4","title":"Key Components:","text":"<ol> <li> <p><code>NERSCTomographyHPCController</code> Class:</p> <ul> <li>This class implements the <code>TomographyHPCController</code> abstract class for the NERSC environment, enabling tomography reconstruction and multi-resolution dataset creation.</li> <li>Methods:<ul> <li><code>create_sfapi_client() -&gt; Client</code>:     Creates and returns a NERSC client instance using the provided credentials for accessing the NERSC SFAPI.</li> <li><code>reconstruct(file_path: str) -&gt; bool</code>:     Starts the tomography reconstruction process at NERSC by submitting a job script to the Perlmutter machine using SFAPI.</li> <li><code>build_multi_resolution(file_path: str) -&gt; bool</code>:     Converts TIFF files to Zarr format on NERSC by submitting a job script to Perlmutter.</li> </ul> </li> </ul> </li> <li> <p><code>schedule_pruning</code> Task:</p> <ul> <li>Schedules the deletion of files from various locations (data832, NERSC) after a specified duration.</li> <li>Parameters:<ul> <li><code>raw_file_path</code>, <code>tiff_file_path</code>, <code>zarr_file_path</code>: Paths to raw, TIFF, and Zarr files.</li> <li>Uses configuration values to determine where to delete the files and when.</li> </ul> </li> <li>Returns: <code>True</code> if all prune tasks were scheduled successfully, <code>False</code> otherwise.</li> </ul> </li> <li> <p><code>nersc_recon_flow</code> Flow:</p> <ul> <li>This is the main flow for performing tomography reconstruction and multi-resolution processing at NERSC. It performs the following steps:<ol> <li>Run tomography reconstruction using NERSC's SFAPI.</li> <li>Run multi-resolution processing to convert TIFF files to Zarr format.</li> <li>Transfer the reconstructed TIFF and Zarr files from NERSC to <code>data832</code>.</li> <li>Schedule pruning tasks to delete processed files from NERSC and data832 after a defined period.</li> </ol> </li> <li> <p>Parameters:  </p> <ul> <li><code>file_path</code>: The path to the file to be processed.</li> <li><code>config</code>: Configuration object containing the necessary endpoints for data transfers.</li> </ul> </li> <li> <p>Returns: <code>True</code> if both reconstruction and multi-resolution tasks were successful, <code>False</code> otherwise.</p> </li> </ul> </li> </ol>"},{"location":"orchestration/#olcfpy","title":"<code>olcf.py</code>","text":"<p>To be implemented ...</p>"}]}