{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d3813a6",
   "metadata": {},
   "source": [
    "# Tomopy Reconstruction Demo for ALS at ALCF\n",
    "\n",
    "This notebook demonstrates how to perform tomography reconstructions using Tomopy for ALS data on Polaris at ALCF using [Globus Flows](https://www.globus.org/globus-flows-service).  In this example, Globus flows will launch the application on Polaris and then transfer results from the Eagle filesystem.\n",
    "\n",
    "This notebook can be run from anywhere, it only requires a local installation of Globus software (described below) and access to a Globus Compute Endpoint setup by the user on Polaris that has access to tomopy (also described below).\n",
    "\n",
    "This demo uses Globus Flows and Globus Compute.  Globus Flows is a reliable and secure platform for orchestrating and performing research data management and analysis tasks. A flow is often needed to manage data coming from instruments, e.g., image files can be moved from local storage attached to a microscope to a high-performance storage system where they may be accessed by all members of the research project.  Globus Compute is a remote executor for tasks expressed as python functions that are sent to remote machines following a fire-and-forget model.\n",
    "\n",
    "In this notebook we will first describe necessary setup tasks for the local environment and on Polaris; second, we will describe how to create and test a Globus Compute function that can remotely launch a tomopy task on Polaris compute nodes; and third, we will describe how to incorporate this function with a Globus Flow that coordinates the execution of the tomopy task with a data transfer step.\n",
    "\n",
    "More examples of creating and running Globus Flows can be found on Globus' [demo instance](https://jupyter.demo.globus.org/hub/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd8e562",
   "metadata": {},
   "source": [
    "## Tomopy on Polaris\n",
    "\n",
    "Tomopy has been installed in a conda environment on Polaris at this path which is accessible to members of the IRIBeta allocation: `/eagle/IRIBeta/als/env/tomopy`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93d4cc1",
   "metadata": {},
   "source": [
    "## Local Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d36389",
   "metadata": {},
   "source": [
    "This notebook can be run from anywhere.  The only requirement is a local environment, such as a conda environment, that has python 3.11 installed along with the Globus packages `globus_compute_sdk` and `globus_cli`.  If you have a local installation of conda you can set up an environment that can run this notebook with these steps:\n",
    "\n",
    "```bash\n",
    "conda create -n globus_env python==3.11\n",
    "conda activate globus_env\n",
    "pip install globus_compute_sdk globus_cli\n",
    "```\n",
    "\n",
    "Note that the tomopy environment on Polaris contains python 3.11. It is therefore necessary for this environment on your local machine to have a python version close to this version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60401bc9",
   "metadata": {},
   "source": [
    "## Create a Globus Compute Endpoint on Polaris"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5f27c7",
   "metadata": {},
   "source": [
    "The first step for a user to execute applications on Polaris through the Globus service is to create a Globus compute endpoint on Polaris.  This requires the user to do a one-time setup task to configure the endpoint.\n",
    "\n",
    "In a shell seperate from this notebook, log into Polaris.  Copy the file included with this notebook called `template_config.yaml` to the Polaris filesystem (doesn't matter where).  Inside `template_config.yaml` you should see options setting your project name (`IRIBeta`), the queue you will use (`debug`), and commands that activate a pre-made conda environment on Polaris that can run tomopy.\n",
    "\n",
    "In your shell on Polaris, execute the following commands:\n",
    "\n",
    "```bash\n",
    "module load conda\n",
    "conda activate /eagle/IRIBeta/als/env/tomopy\n",
    "globus-compute-endpoint configure --endpoint-config template_config.yaml als_endpoint\n",
    "globus-compute-endpoint start als_endpoint\n",
    "globus-compute-endpoint list\n",
    "```\n",
    "This will create an endpoint and display its status.  Its status should be listed as `running`.  There will also be displayed a unique Endpoint ID in the form of a UUID.  Copy that ID and paste it below as a string assigned to `YOUR_ENDPOINT_FROM_ABOVE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decc84d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_ENDPOINT_FROM_ABOVE = \"UUID-HERE\"\n",
    "polaris_endpoint_id = YOUR_ENDPOINT_FROM_ABOVE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbac1fca",
   "metadata": {},
   "source": [
    "Your endpoint is now active as a daemon process running on the Polaris login node.  It is communicating with the Globus service and waiting for work.  If you ever want to stop the process you can run:\n",
    "```bash\n",
    "globus-compute-endpoint stop als_endpoint\n",
    "```\n",
    "Your process may need to be periodically restarted, for example after Polaris comes back from a maintance period.\n",
    "\n",
    "If you ever need to make changes to your endpoint configuration, you can find the settings file in `~/.globus_compute/als_endpoint/config.yaml`.  Edit this file and then restart the endpoint with `globus-compute-endpoint restart als_endpoint` to make the changes active.\n",
    "\n",
    "This endpoint will submit work to the `debug` queue since this demo is for learning purposes.  In production, ALS will be able to submit work to the [demand queue](https://docs.alcf.anl.gov/polaris/running-jobs/#queues) which will give immediate access to Polaris compute nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38640cb7",
   "metadata": {},
   "source": [
    "## Create a Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22d0caf",
   "metadata": {},
   "source": [
    "We first need to create a python function that wraps around the application call.  We will call it `reconstruction_wrapper` that takes as an input the directory on the eagle file system where the input data are located, `rundir`, and the name of the input `parametersfile`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a08c0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original code\n",
    "def reconstruction_wrapper(rundir, parametersfile=\"inputOneSliceOfEach.txt\"):\n",
    "    import os\n",
    "    import time\n",
    "    import subprocess\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Move to directory where data are located\n",
    "    os.chdir(rundir)\n",
    "\n",
    "    # Run reconstruction.py\n",
    "    command = f\"python /eagle/IRIBeta/als/example/reconstruction.py {parametersfile}\"\n",
    "    res = subprocess.run(command.split(\" \"), stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "    end = time.time()\n",
    "    \n",
    "    return f\"Reconstructed data specified in {parametersfile} in {end-start} seconds;\\n {res}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6987383d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated script for globus_reconstruction.py\n",
    "def reconstruction_wrapper(rundir, file_name, folder_path):\n",
    "    import os\n",
    "    import time\n",
    "    import subprocess\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Move to directory where data are located\n",
    "    os.chdir(rundir)\n",
    "\n",
    "    # Run reconstruction.py\n",
    "    command = f\"python /eagle/IRIBeta/als/example/globus_reconstruction.py {file_name} {folder_path}\"\n",
    "    res = subprocess.run(command.split(\" \"), stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "    end = time.time()\n",
    "    \n",
    "    return f\"Reconstructed data specified in {folder_path} / {file_name} in {end-start} seconds;\\n {res}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9b2265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated flow for globus_reconstruction.py + zarr generation\n",
    "def reconstruction_wrapper(rundir, h5_file_name, folder_path):\n",
    "    import os\n",
    "    import time\n",
    "    import subprocess\n",
    "\n",
    "    rec_start = time.time()\n",
    "\n",
    "    # Move to directory where data are located\n",
    "    os.chdir(rundir)\n",
    "\n",
    "    # Run reconstruction.py\n",
    "    command = f\"python /eagle/IRIBeta/als/example/globus_reconstruction.py {h5_file_name} {folder_path}\"\n",
    "    recon_res = subprocess.run(command.split(\" \"), stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "    rec_end = time.time()\n",
    "    \n",
    "    print(f\"Reconstructed data specified in {folder_path} / {h5_file_name} in {rec_end-rec_start} seconds;\\n {recon_res}\")\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Convert tiff files to zarr\n",
    "    file_name = h5_file_name[:-3] if h5_file_name.endswith('.h5') else h5_file_name\n",
    "    command = f\"python /eagle/IRIBeta/als/example/tiff_to_zarr.py /eagle/IRIBeta/als/bl832/scratch/{folder_path}/rec{file_name}/\" # --zarr_directory /path/to/storage/\"\n",
    "    zarr_res = subprocess.run(command.split(\" \"), stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"Converted tiff files to zarr in {end-start} seconds;\\n {zarr_res}\")\n",
    "\n",
    "    return f\"Reconstructed data specified in {folder_path} / {h5_file_name} in {rec_end-rec_start} seconds;\\n {recon_res} \\n Converted tiff files to zarr in {end-start} seconds;\\n {zarr_res}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b6085e",
   "metadata": {},
   "source": [
    "This notebook can be run from anywhere.  The only requirement is a local environment, such as a conda environment, that has python 3.11 installed along with the Globus packages `globus_compute_sdk` and `globus_cli`.  If you have a local installation of conda you can set up an environment that can run this notebook with these steps:\n",
    "\n",
    "```bash\n",
    "conda create -n globus_env python==3.11\n",
    "conda activate globus_env\n",
    "pip install globus_compute_sdk globus_cli python-dotenv\n",
    "```\n",
    "\n",
    "Note that the tomopy environment on Polaris contains python 3.11. It is therefore necessary for this environment on your local machine to have a python version close to this version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caae3a4e",
   "metadata": {},
   "source": [
    "## Authenticate Client and Test Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764af861",
   "metadata": {},
   "source": [
    "We will now instantiate a Globus Compute client to test the function.  Globus will prompt the user for their credentials if running for the first time.  The user should have a Globus account through their ALCF account and should validate with their ALCF credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from globus_compute_sdk import Client, Executor\n",
    "import os\n",
    "os.environ[\"GLOBUS_COMPUTE_CLIENT_ID\"] = os.getenv(\"GLOBUS_CLIENT_ID\")\n",
    "os.environ[\"GLOBUS_COMPUTE_CLIENT_SECRET\"] = os.getenv(\"GLOBUS_CLIENT_SECRET\")\n",
    "\n",
    "gc = Client()\n",
    "\n",
    "polaris_endpoint_id = YOUR_ENDPOINT_FROM_ABOVE\n",
    "gce = Executor(endpoint_id=polaris_endpoint_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67262fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original code for submitting the job\n",
    "future = gce.submit(reconstruction_wrapper, \"/eagle/IRIBeta/als/example\")\n",
    "print(future.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32da123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated code for globus_reconstruction.py\n",
    "future = gce.submit(reconstruction_wrapper, \"/eagle/IRIBeta/als/bl832/raw\", \"20230224_132553_sea_shell.h5\", \"BLS-00564_dyparkinson\")\n",
    "print(future.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook can be run from anywhere.  The only requirement is a local environment, such as a conda environment, that has python 3.11 installed along with the Globus packages `globus_compute_sdk` and `globus_cli`.  If you have a local installation of conda you can set up an environment that can run this notebook with these steps:\n",
    "\n",
    "```bash\n",
    "conda create -n globus_env python==3.11\n",
    "conda activate globus_env\n",
    "pip install globus_compute_sdk globus_cli python-dotenv\n",
    "```\n",
    "\n",
    "Note that the tomopy environment on Polaris contains python 3.11. It is therefore necessary for this environment on your local machine to have a python version close to this version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5f8ed9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae9b20d4",
   "metadata": {},
   "source": [
    "## Register Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcefda2",
   "metadata": {},
   "source": [
    "Now that the function has been tested and works, register the function with the Globus service.  This will allow the user to call the function from within a flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742fd4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_func = gc.register_function(reconstruction_wrapper)\n",
    "\n",
    "print(reconstruction_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34c26d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original code\n",
    "future = gce.submit_to_registered_function(args=[\"/eagle/IRIBeta/als/example\"], function_id=reconstruction_func)\n",
    "future.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7309d272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated code for globus_reconstruction.py\n",
    "future = gce.submit_to_registered_function(args=[\"/eagle/IRIBeta/als/bl832/raw\", \"20230224_132553_sea_shell.h5\", \"BLS-00564_dyparkinson\"], function_id=reconstruction_func)\n",
    "future.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a51665b",
   "metadata": {},
   "source": [
    "## Incorporate Function into a Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90530096",
   "metadata": {},
   "source": [
    "Now we will incorporate the Tomopy function into an example flow to run Tomopy on Polaris in coordination with other tasks.\n",
    "\n",
    "This example simply includes two steps:\n",
    "1. Run Tomopy via Globus Compute\n",
    "2. Transfer results from the eagle file system to the home file system.\n",
    "\n",
    "This can easily be extended to include other steps to import data, perform postprocessing, or publish and catalog results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c405e165",
   "metadata": {},
   "source": [
    "This is the flow definition for this two-step flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a972fafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_definition = {\n",
    "    \"Comment\": \"Run Reconstruction and transfer results\",\n",
    "    \"StartAt\": \"Reconstruction\",\n",
    "    \"States\": {\n",
    "        \"Reconstruction\": {\n",
    "            \"Comment\": \"Reconstruction with Tomopy\",\n",
    "            \"Type\": \"Action\",\n",
    "            \"ActionUrl\": \"https://compute.actions.globus.org/fxap\",\n",
    "            \"Parameters\": {\n",
    "                \"endpoint.$\": \"$.input.compute_endpoint_id\",\n",
    "                \"function.$\": \"$.input.compute_function_id\",\n",
    "                \"kwargs.$\": \"$.input.compute_function_kwargs\"\n",
    "            },\n",
    "            \"ResultPath\": \"$.ReconstructionOutput\",\n",
    "            \"WaitTime\": 3600,\n",
    "            \"Next\": \"Transfer_Out\"\n",
    "        },\n",
    "        \"Transfer_Out\": {\n",
    "            \"Comment\": \"Transfer files\",\n",
    "            \"Type\": \"Action\",\n",
    "            \"ActionUrl\": \"https://actions.automate.globus.org/transfer/transfer\",\n",
    "            \"Parameters\": {\n",
    "                \"source_endpoint_id.$\": \"$.input.source.id\",\n",
    "                \"destination_endpoint_id.$\": \"$.input.destination.id\",\n",
    "                \"transfer_items\": [\n",
    "                    {\n",
    "                        \"source_path.$\": \"$.input.source.path\",\n",
    "                        \"destination_path.$\": \"$.input.destination.path\",\n",
    "                        \"recursive.$\": \"$.input.recursive_tx\"\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            \"ResultPath\": \"$.TransferFiles\",\n",
    "            \"WaitTime\": 300,\n",
    "            \"End\": True\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1811d7a0",
   "metadata": {},
   "source": [
    "Next, we need to provide a flows client id to run the flow.  For now we will use the demo client id, but a project should create a client for their work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137cbf00",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3efe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import globus_sdk\n",
    "\n",
    "# from utils import get_flows_client, get_specific_flow_client\n",
    "from orchestration.globus.flows import get_flows_client, get_specific_flow_client\n",
    "\n",
    "# Tutorial client ID\n",
    "# We recommend replacing this with your own client for any production use-cases\n",
    "# Create your own at developers.globus.org\n",
    "CLIENT_ID = \"UUID-HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbffcd67",
   "metadata": {},
   "source": [
    "Now get an instance of the flows client.  You will be asked to validate credentials with the Globus service.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee782e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = get_flows_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680519c8",
   "metadata": {},
   "source": [
    "Next, create a flow.  You will again be asked to validate credentials with the globus service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0fb24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = fc.create_flow(definition=flow_definition, title=\"Reconstruction flow\", input_schema={})\n",
    "flow_id = flow['id']\n",
    "print(flow)\n",
    "flow_scope = flow['globus_auth_scope']\n",
    "print(f'Newly created flow with id:\\n{flow_id}\\nand scope:\\n{flow_scope}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b146f619",
   "metadata": {},
   "source": [
    "## Run the Flow\n",
    "\n",
    "Now we need to create a set of inputs for the flow, that will follow the json structure below.  The key elements that are needed are:\n",
    "1. The Globus compute endpoint id\n",
    "2. The Globus compute function id\n",
    "3. Inputs to the Globus compute function\n",
    "4. The endpoint and path from which to transfer data\n",
    "5. The endpoint and path to which to transfer data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca578c2",
   "metadata": {},
   "source": [
    "The user should choose a destination path and endpoint that they have access to.  As a sample endpoint, below the ALCF /home space is used.  To use this endpoint, the user should set a path in their home space (but remove the leading `/home` so that it will appear like `/csimpson/als_example`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6056aeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "alcfhome_transfer_endpoint_id = \"UUID-HERE\"\n",
    "destination_path_on_alcfhome = \"/your-path-here\" # Note that paths for transfers on the home endpoint should remove the leading /home"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea4510b",
   "metadata": {},
   "source": [
    "The example problem is setup in a directory on the Eagle filesystem at ALCF, so that endpoint and path is used as the source of the data.  All the endpoints and functions are added to a `flow_input`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee0f949",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_endpoint = \"UUID-HERE\"\n",
    "reconstruction_func = \"UUID-HERE\"\n",
    "\n",
    "eagle_transfer_endpoint_id = \"UUID-HERE\"\n",
    "source_path_on_eagle = \"/IRIBeta/als/example\" # Note that paths for transfers on the eagle endpoint should remove the leading /eagle\n",
    "\n",
    "function_inputs = {\"rundir\": \"/eagle/IRIBeta/als/bl832/raw\", \"file_name\": \"20230224_132553_sea_shell.h5\", \"folder_path\": \"BLS-00564_dyparkinson\"}\n",
    "\n",
    "flow_input = {\n",
    "    \"input\": {\n",
    "      \"source\": {\n",
    "        \"id\": collection_endpoint,\n",
    "        \"path\": \"/bl832/raw/BLS-00564_dyparkinson\"\n",
    "      },\n",
    "      \"destination\": {\n",
    "        \"id\": collection_endpoint,\n",
    "        \"path\": \"/bl832tch/BLS-00564_dyparkinson\"\n",
    "      },\n",
    "      \"recursive_tx\": True,\n",
    "      \"compute_endpoint_id\": polaris_endpoint_id,\n",
    "      \"compute_function_id\": reconstruction_func,\n",
    "      \"compute_function_kwargs\": function_inputs\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc12cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collection_ids should contain all the transfer endpoint ids involved in the flow\n",
    "collection_ids = [flow_input[\"input\"][\"source\"][\"id\"], flow_input[\"input\"][\"destination\"][\"id\"]]\n",
    "flow_id = \"UUID-HERE\"\n",
    "fc = get_flows_client()\n",
    "flow_client = get_specific_flow_client(flow_id, collection_ids=collection_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2635cf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_action = flow_client.run_flow(flow_input, label=\"ALS run\", tags=[\"demo\", \"als\", \"tomopy\"])\n",
    "flow_run_id = flow_action['action_id']\n",
    "\n",
    "print(f'Flow action started with id: {flow_run_id}')\n",
    "\n",
    "print(f\"Monitor your flow here: https://app.globus.org/runs/{flow_run_id}\")\n",
    "\n",
    "flow_status = flow_action['status']\n",
    "# while flow_status in ['ACTIVE', 'INACTIVE']:\n",
    "#     time.sleep(10)\n",
    "#     flow_action = fc.get_run(flow_run_id)\n",
    "#     flow_status = flow_action['status']\n",
    "#     print(f'Flow status: {flow_status}')\n",
    "# print(f'Final status: {flow_status}')\n",
    "while flow_status in ['ACTIVE', 'INACTIVE']:\n",
    "    time.sleep(10)\n",
    "    flow_action = fc.get_run(flow_run_id)\n",
    "    flow_status = flow_action['status']\n",
    "    print(f'Updated flow status: {flow_status}')\n",
    "    # Log additional details about the flow status\n",
    "    print(f'Flow action details: {flow_action}')\n",
    "\n",
    "if flow_status != 'SUCCEEDED':\n",
    "    print(f'Flow failed with status: {flow_status}')\n",
    "    # Log additional details about the failure\n",
    "    print(f'Flow failure details: {flow_action}')\n",
    "else:\n",
    "    print(f'Flow completed successfully with status: {flow_status}')\n",
    "    success = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1778c51",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Additional steps to the flow can be included if needed (for example an initial transfer step before tomopy is run).  The flow can also be adapted to be executed through the Globus web UI by adding a schema.  More information on flows can be found [here](https://docs.globus.org/guides/tutorials/flow-automation/create-a-flow/).\n",
    "\n",
    "The Globus compute endpoint configuration can be adapted to the user's needs.  Multiple instances of tomopy can be run in parallel on each node by adapting the `config.yaml` file.\n",
    "\n",
    "The content in this notebook can also be adapted to be run in a python script or a bash script.  More information can be found in the documentation for the [globus python api](https://globus-sdk-python.readthedocs.io/en/stable/services/flows.html) or the documentation for the [cli api](https://docs.globus.org/cli/reference/#globus_flows_commands)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
